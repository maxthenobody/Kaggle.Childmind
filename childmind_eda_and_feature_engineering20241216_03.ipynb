{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_rank\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_timeline\n",
    "\n",
    "import shap\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "pl.Config.set_tbl_rows(-1)\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_fmt_str_lengths(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import display as ipydisplay, HTML\n",
    "#ipydisplay(HTML(\"<style>.jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { max-height: 80em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"I:/Kaggle/child-mind-institute-problematic-internet-use/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path + 'train.csv', dtype={'id': str})\n",
    "test = pd.read_csv(path + 'test.csv', dtype={'id': str})\n",
    "#sample = pd.read_csv(path + 'sample_submission.csv', dtype={'id': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = pd.read_csv(path + 'data_dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train[test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([train_target, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df['total_num_nan'] = concat_df.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_target\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = concat_df[concat_df['total_num_nan'] == concat_df['total_num_nan'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in concat_df.columns:\n",
    "    if col == 'id':\n",
    "        continue\n",
    "    new_col_name = col + '_isnan'\n",
    "    concat_df[new_col_name] = concat_df[col].isna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = data_dict['Instrument'].unique().tolist()\n",
    "instruments.remove('Identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instrument in instruments:\n",
    "    if 'Parent-Child Internet Addiction Test' == instrument:\n",
    "        continue\n",
    "    instrument_df = data_dict[data_dict['Instrument'] == instrument]\n",
    "    field_list = instrument_df['Field'].unique().tolist()\n",
    "    concat_df[instrument + '_num_nan'] = concat_df[field_list].isna().sum(axis=1)\n",
    "\n",
    "del instrument_df, field_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_parquet_id_folders_list = os.listdir(path + 'series_train.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_parquet_id_list = [v.replace('id=', '') for v in train_parquet_id_folders_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parquet_id_folders_list = os.listdir(path + 'series_test.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parquet_id_list = [v.replace('id=', '') for v in test_parquet_id_folders_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp_df = concat_df.head(train.shape[0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp_df = concat_df.tail(test.shape[0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del concat_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp_df['has_parquet'] = train_temp_df['id'].isin(train_parquet_id_list).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_temp_df['has_parquet'] = test_temp_df['id'].isin(test_parquet_id_list).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/antoninadolgorukova/cmi-piu-actigraphy-data-eda\n",
    "entropy = lambda x: -(x / x.sum() * np.log(x / x.sum() + 1e-9)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_engi(df, target_cols):\n",
    "    stats_df = df[target_cols].describe(percentiles=[0.01, 0.03] + [(i+1)*0.05 for i in range(19)] + [0.97, 0.99]).T\n",
    "    count = stats_df['count'][0]\n",
    "    stats_df.drop('count', axis=1, inplace=True)\n",
    "    \n",
    "    column_names_list = []\n",
    "    for idx in stats_df.index:\n",
    "        for col in stats_df.columns:\n",
    "            column_names_list.append(str(idx) + '_' + str(col))\n",
    "\n",
    "    reshaped_df = pd.DataFrame(stats_df.values.reshape(-1)).T.reset_index(drop=True)\n",
    "    reshaped_df.columns = column_names_list\n",
    "\n",
    "    reshaped_df['count'] = count\n",
    "\n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_feat_engi(df, col_to_group, target_col):\n",
    "    stats_df = pd.DataFrame(\n",
    "        df.groupby(col_to_group)[target_col].describe(percentiles=[0.01, 0.03] + [(i+1)*0.05 for i in range(19)] + [0.97, 0.99])\n",
    "    )\n",
    "\n",
    "    stats_df['sum'] = df.groupby(col_to_group)[target_col].sum()\n",
    "    stats_df['entropy'] = df.groupby(col_to_group)[target_col].apply(entropy)\n",
    "    \n",
    "    column_names_list = []\n",
    "    for idx in stats_df.index:\n",
    "        for col in stats_df.columns:\n",
    "            column_names_list.append(target_col + '_' + col_to_group + '_' + str(idx) + '_' + str(col))\n",
    "\n",
    "    reshaped_df = pd.DataFrame(stats_df.values.reshape(-1)).T.reset_index(drop=True)\n",
    "    reshaped_df.columns = column_names_list\n",
    "\n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncommon_groupby_feat_engi(df, col_to_group, target_col):\n",
    "    stats_df = pd.DataFrame(\n",
    "        df.groupby(col_to_group)[target_col].describe()\n",
    "        #df.groupby(col_to_group)[target_col].describe(percentiles=[0.01, 0.03] + [(i+1)*0.05 for i in range(19)] + [0.97, 0.99])\n",
    "    )\n",
    "\n",
    "    stats_df['sum'] = df.groupby(col_to_group)[target_col].sum()\n",
    "    stats_df['entropy'] = df.groupby(col_to_group)[target_col].apply(entropy)\n",
    "\n",
    "    \n",
    "    #stats_stats_df = stats_df.describe(percentiles=[0.01, 0.03] + [(i+1)*0.05 for i in range(19)] + [0.97, 0.99])\n",
    "    stats_stats_df = stats_df.describe()\n",
    "    stats_stats_count = stats_stats_df['count'][0]\n",
    "    stats_stats_df.drop('count', axis=0, inplace=True)\n",
    "    stats_stats_df = stats_stats_df.T\n",
    "    \n",
    "    column_names_list = []\n",
    "    for idx in stats_stats_df.index:\n",
    "        for col in stats_stats_df.columns:\n",
    "            column_names_list.append(target_col + '_' + col_to_group + '_' + str(idx) + '_' + str(col))\n",
    "    \n",
    "    reshaped_df = pd.DataFrame(stats_stats_df.values.reshape(-1)).T.reset_index(drop=True)\n",
    "    reshaped_df.columns = column_names_list\n",
    "\n",
    "    current_cols = reshaped_df.columns.tolist()\n",
    "    count_col_name = target_col + '_' + col_to_group + '_count'\n",
    "    reshaped_df[count_col_name] = stats_stats_count\n",
    "\n",
    "    reshaped_df = reshaped_df[[count_col_name] + current_cols]\n",
    "\n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(dataset='train'):\n",
    "    parquet_id_folders_list = os.listdir(path + 'series_' + dataset + '.parquet/')\n",
    "    ts_list = []\n",
    "    ts_features_list = []\n",
    "    for id_folder in tqdm(parquet_id_folders_list):\n",
    "        iid = id_folder.replace('id=', '')\n",
    "    \n",
    "        parquet_files_path = path + 'series_' + dataset + '.parquet/' + id_folder + '/'\n",
    "        parquet_files_list = os.listdir(parquet_files_path)\n",
    "    \n",
    "        dfs_list = []\n",
    "        for parquet_filename in parquet_files_list:\n",
    "            if '.parquet' in parquet_filename:\n",
    "                parquet_df = pd.read_parquet(parquet_files_path + parquet_filename)\n",
    "                dfs_list.append(parquet_df)\n",
    "        parquet_concat_df = pd.concat(dfs_list, ignore_index=True)\n",
    "        current_cols = parquet_concat_df.columns.tolist()\n",
    "        \n",
    "        parquet_concat_df['id'] = iid\n",
    "        \n",
    "        parquet_concat_df = parquet_concat_df[['id'] + current_cols]\n",
    "        \n",
    "        parquet_concat_df['day_since_wear'] = (parquet_concat_df['relative_date_PCIAT'] - parquet_concat_df['relative_date_PCIAT'].min()).astype(int)\n",
    "        parquet_concat_df['time_of_day_sec'] = parquet_concat_df['time_of_day'] / 1e9\n",
    "        parquet_concat_df.drop('time_of_day', axis=1, inplace=True)\n",
    "        parquet_concat_df['time_of_day_min'] = parquet_concat_df['time_of_day_sec'] / 60\n",
    "        parquet_concat_df['time_of_day_hour'] = parquet_concat_df['time_of_day_min'] / 60\n",
    "        parquet_concat_df['time_of_day_day'] = parquet_concat_df['time_of_day_hour'] / 24\n",
    "        parquet_concat_df['time_of_day_hour_window'] = parquet_concat_df['time_of_day_hour'].astype(int)\n",
    "        parquet_concat_df['time_of_day_min_window'] = parquet_concat_df['time_of_day_min'].astype(int)\n",
    "        parquet_concat_df['time_of_day_15_min_window'] = (parquet_concat_df['time_of_day_min'] / 15).astype(int)\n",
    "        parquet_concat_df['timestamp_day'] = parquet_concat_df['day_since_wear'] + parquet_concat_df['time_of_day_day']\n",
    "    \n",
    "        if parquet_concat_df['timestamp_day'].nunique() != parquet_concat_df.shape[0]:\n",
    "            print('yes')\n",
    "    \n",
    "        parquet_concat_df['timestamp_hour'] = parquet_concat_df['timestamp_day'] * 24\n",
    "        parquet_concat_df['timestamp_min'] = parquet_concat_df['timestamp_day'] * 24 * 60\n",
    "        parquet_concat_df['timestamp_sec'] = parquet_concat_df['timestamp_day'] * 24 * 60 * 60\n",
    "        parquet_concat_df['timestamp_15_min'] = parquet_concat_df['timestamp_day'] * 24 * (60/15)\n",
    "    \n",
    "        parquet_concat_df['timestamp_hour_window'] = parquet_concat_df['timestamp_hour'].astype(int)\n",
    "        parquet_concat_df['timestamp_min_window'] = parquet_concat_df['timestamp_min'].astype(int)\n",
    "        parquet_concat_df['timestamp_15_min_window'] = parquet_concat_df['timestamp_15_min'].astype(int)\n",
    "    \n",
    "        battery_voltage_start = parquet_concat_df['battery_voltage'][0]\n",
    "        parquet_concat_df['battery_use_since_wear'] = -(parquet_concat_df['battery_voltage'] - battery_voltage_start)\n",
    "        \n",
    "    \n",
    "        worn_df = parquet_concat_df[parquet_concat_df['non-wear_flag'] == 0].reset_index(drop=True)\n",
    "    \n",
    "        \n",
    "        feature_cols = ['X', 'Y', 'Z', 'enmo', 'anglez', 'light', 'battery_use_since_wear']\n",
    "    \n",
    "        features_df = feat_engi(worn_df, feature_cols)\n",
    "    \n",
    "        groupby_cols = ['weekday', 'time_of_day_hour_window']#, 'time_of_day_15_min_window']\n",
    "    \n",
    "        groupby_df_list = []\n",
    "        for groupby_col in groupby_cols:\n",
    "            for feature_col in feature_cols:\n",
    "                groupby_features_df = groupby_feat_engi(worn_df, groupby_col, feature_col)\n",
    "                groupby_df_list.append(groupby_features_df)\n",
    "    \n",
    "        groupby_concat_df = pd.concat(groupby_df_list, axis=1)\n",
    "    \n",
    "        uncommon_groupby_cols = ['day_since_wear', 'timestamp_hour_window']#, 'timestamp_15_min_window']#, 'timestamp_min_window']\n",
    "    \n",
    "        uncommon_groupby_df_list = []\n",
    "        for uncommon_groupby_col in uncommon_groupby_cols:\n",
    "            for unc_feature_col in feature_cols:\n",
    "                uncommon_groupby_df = uncommon_groupby_feat_engi(worn_df, uncommon_groupby_col, unc_feature_col)\n",
    "                uncommon_groupby_df_list.append(uncommon_groupby_df)\n",
    "    \n",
    "        features_concat_df = pd.concat([features_df, groupby_concat_df, uncommon_groupby_df], axis=1)\n",
    "        features_current_cols = features_concat_df.columns.tolist()\n",
    "        features_concat_df['id'] = iid\n",
    "        features_concat_df = features_concat_df[['id'] + features_current_cols]\n",
    "    \n",
    "        for q in [1,2,3,4]:\n",
    "            if q in worn_df['quarter'].tolist():\n",
    "                features_concat_df['data_collected_in_quarter_' + str(q)] = 1\n",
    "            else:\n",
    "                features_concat_df['data_collected_in_quarter_' + str(q)] = 0\n",
    "\n",
    "        features_concat_df['id'] = features_concat_df['id'].astype(str)\n",
    "\n",
    "        #features_concat_df.to_csv(path + 'series_' + dataset + '.parquet/id=' + iid + '/features.csv', index=False)\n",
    "        \n",
    "        ts_list.append(parquet_concat_df)\n",
    "        ts_features_list.append(features_concat_df)\n",
    "\n",
    "        gc.collect()\n",
    "    gc.collect()\n",
    "    \n",
    "    return ts_list, ts_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features_files():\n",
    "    temp_dfs_list = []\n",
    "    ts_features_folders_list = os.listdir(path + 'train_ts_features/')\n",
    "    for temp_id_folder in tqdm(ts_features_folders_list):\n",
    "        temp_files_list = os.listdir(path + 'train_ts_features/' + temp_id_folder + '/')\n",
    "        for temp_filename in temp_files_list:\n",
    "            if '.csv' in temp_filename:\n",
    "                temp_df = pd.read_csv(path + 'train_ts_features/' + temp_id_folder + '/' + temp_filename, dtype={'id': str})\n",
    "                temp_dfs_list.append(temp_df)\n",
    "    gc.collect()\n",
    "    return temp_dfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ts_features_list = read_features_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ts_list, test_ts_features_list = read_parquet(dataset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts_features_concat_df = pd.concat(train_ts_features_list, ignore_index=True)\n",
    "test_ts_features_concat_df = pd.concat(test_ts_features_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_ts_features_list, test_ts_features_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merge_df = train_temp_df.merge(train_ts_features_concat_df, how='left', on='id')\n",
    "test_merge_df = test_temp_df.merge(test_ts_features_concat_df, how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_ts_features_concat_df, test_ts_features_concat_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "for i in range(data_dict.shape[0]):\n",
    "    col = data_dict['Field'][i]\n",
    "    if col == 'id':\n",
    "        continue\n",
    "    if (data_dict['Type'][i] == 'str') | ('categori' in data_dict['Type'][i]):\n",
    "        cat_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_variables = ['FGC-FGC_CU_Zone', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU_Zone',\n",
    "                     'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num',\n",
    "                     'PreInt_EduHx-computerinternet_hoursday']\n",
    "\n",
    "ordinal_variables += [col for col in cat_cols if (('PCIAT' in col)&('Season' not in col))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(set(cat_cols) - set(ordinal_variables))\n",
    "cat_cols = [col for col in cat_cols if 'PCIAT' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_concat_df = pd.concat([train_merge_df, test_merge_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_merge_df, test_merge_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pciat_cols(df):\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if 'PCIAT' in col:\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_concat_df = remove_pciat_cols(merge_concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_cat_cols(df):\n",
    "    for col in tqdm(cat_cols):\n",
    "        df[col] = df[col].fillna('Missing').apply(str)\n",
    "        unique_list = sorted(df[col].unique().tolist())\n",
    "\n",
    "        for v in unique_list:\n",
    "            new_col_name = col + '_' + v\n",
    "            df[new_col_name] = (df[col] == v).astype(int)\n",
    "    \n",
    "    df = df.drop(cat_cols, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_concat_df = one_hot_cat_cols(merge_concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "def feature_engineering(df):\n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_concat_df = feature_engineering(merge_concat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_id_cols = [col for col in merge_concat_df.columns if col != 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(np.isinf(merge_concat_df[no_id_cols])):\n",
    "    print('inf in data')\n",
    "    merge_concat_df[no_id_cols] = merge_concat_df[no_id_cols].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "merge_concat_df[no_id_cols] = imputer.fit_transform(merge_concat_df[no_id_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del imputer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = merge_concat_df.head(train.shape[0]).reset_index(drop=True)\n",
    "test_df = merge_concat_df.tail(test.shape[0]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sii'] = train['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nonan_df = train_df[~train_df['sii'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#selected_features_df = pd.read_csv(path + 'selected_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_cols = selected_features_df['selected_features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del selected_features_df\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_cols = train_cols.copy()\n",
    "#test_cols.remove('sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_final_df = train_nonan_df[train_cols].copy()\n",
    "#test_final_df = test_df[test_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df = train_nonan_df.copy()\n",
    "test_final_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_nonan_df, test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#selected_cols_df = pd.read_csv(path + 'selected_cols.csv')\n",
    "#selected_cols_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_cols = selected_cols_df['col'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_final_df = train_final_df[selected_cols + ['sii']]\n",
    "#print(train_final_df.shape)\n",
    "#train_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_final_df = test_final_df[selected_cols]\n",
    "#print(test_final_df.shape)\n",
    "#test_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = train_final_df.drop(['id', 'sii'], axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features with high correlation\n",
    "high_corr = np.where(corr_df > 0.9)\n",
    "high_corr_pairs = [\n",
    "    (corr_df.index[x], corr_df.columns[y])\n",
    "    for x, y in zip(*high_corr)\n",
    "    if x != y and x < y\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_v_list = []\n",
    "for v in high_corr_pairs:\n",
    "    second_v = v[1]\n",
    "    second_v_list.append(second_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_features = list(set(second_v_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final_df = train_final_df.drop(redundant_features, axis=1)\n",
    "print(train_final_df.shape)\n",
    "train_final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final_df = test_final_df.drop(redundant_features, axis=1)\n",
    "print(test_final_df.shape)\n",
    "test_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter tuning, training, prediction & submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "# Edited by Max (Jongyun Han)\n",
    "def catboost_param_tune(train_data, test_data, optuna_n_trials):\n",
    "    X = train_data.drop(['id', 'sii'], axis=1)\n",
    "    y = train_data['sii']\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "     \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "\n",
    "    base_params = {\n",
    "        'iterations': 1000,\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'task_type': 'GPU',\n",
    "        'use_best_model': True,\n",
    "        'boosting_type': 'Plain'\n",
    "    }\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "        \n",
    "        params_to_tune = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            #'depth': trial.suggest_int('depth', 6, 7),\n",
    "            'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 1000),\n",
    "            'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 10, 50),\n",
    "            #'border_count': trial.suggest_int('border_count', 64, 512),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 20),\n",
    "            'random_strength': trial.suggest_float('random_strength', 0, 10)\n",
    "        }\n",
    "    \n",
    "        #for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "            model = CatBoostRegressor(\n",
    "                **base_params,\n",
    "                **params_to_tune\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))#, plot=True)\n",
    "    \n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "    \n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        # https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/discussion/551533\n",
    "        y_preds_df = pd.DataFrame({'y':y, 'preds':oof_non_rounded})\n",
    "        oof_initial_thresholds = y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=oof_initial_thresholds, args=(y, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "        del y_preds_df, oof_initial_thresholds\n",
    "        gc.collect()\n",
    "    \n",
    "        return tKappa\n",
    "\n",
    "    with tqdm(total=optuna_n_trials, desc=\"Optimizing\", unit=\"trial\") as pbar:\n",
    "        \n",
    "        # Define a callback function to update the progress bar\n",
    "        def progress_bar_callback(study, trial):\n",
    "            pbar.update(1)\n",
    "    \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=optuna_n_trials, callbacks=[progress_bar_callback])\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "    shap_importance_list = []\n",
    "\n",
    "    #for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "    for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            **base_params,\n",
    "            **best_params\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), plot=True)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_y_preds_df = pd.DataFrame({'y':y_train, 'preds':y_train_pred})\n",
    "        train_oof_initial_thresholds = train_y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "\n",
    "        train_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=train_oof_initial_thresholds, args=(y_train, y_train_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert train_KappaOPtimizer.success, \"Train Optimization did not converge.\"\n",
    "\n",
    "        train_pred_tuned = threshold_Rounder(y_train_pred, train_KappaOPtimizer.x)\n",
    "        train_Kappa = quadratic_weighted_kappa(y_train, train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_y_preds_df = pd.DataFrame({'y':y_val, 'preds':y_val_pred})\n",
    "        val_oof_initial_thresholds = val_y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "\n",
    "        val_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=val_oof_initial_thresholds, args=(y_val, y_val_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert val_KappaOPtimizer.success, \"Val Optimization did not converge.\"\n",
    "\n",
    "        val_pred_tuned = threshold_Rounder(y_val_pred, val_KappaOPtimizer.x)\n",
    "        val_Kappa = quadratic_weighted_kappa(y_val, val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        train_optimized_val_pred_tuned = threshold_Rounder(y_val_pred, train_KappaOPtimizer.x)\n",
    "        train_optimized_val_Kappa = quadratic_weighted_kappa(y_val, train_optimized_val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized Val QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_optimized_val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_optimized_train_pred_tuned = threshold_Rounder(y_train_pred, val_KappaOPtimizer.x)\n",
    "        val_optimized_train_Kappa = quadratic_weighted_kappa(y_train, val_optimized_train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized Train QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_optimized_train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        rounded_train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        rounded_val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)        \n",
    "\n",
    "        train_S.append(train_Kappa)\n",
    "        test_S.append(val_Kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Rounded Train QWK: {rounded_train_kappa:.4f}, Rounded Validation QWK: {rounded_val_kappa:.4f}\")\n",
    "        #clear_output(wait=True)\n",
    "\n",
    "        models_list.append(model)\n",
    "\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X=X_val, y=y_val)\n",
    "        shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "        shap_importance_list.append(shap_importance)\n",
    "\n",
    "        \n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    y_preds_df = pd.DataFrame({'y':y, 'preds':oof_non_rounded})\n",
    "    oof_initial_thresholds = y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=oof_initial_thresholds, args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data_copy['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return models_list, submission, oof_non_rounded, tpm, study, np.mean(shap_importance_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_models_list, cat_submission_df, cat_raw_train_preds, cat_raw_test_preds, cat_optuna_study, cat_shap_importance = catboost_param_tune(\n",
    "    train_final_df, test_final_df, 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in cat_optuna_study.best_params.keys():\n",
    "    fig = plot_slice(cat_optuna_study, params=[param])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(cat_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_optuna_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_final_df.drop(['id', 'sii'], axis=1).columns\n",
    "imp_df = pd.DataFrame(sorted(zip(cols, cat_shap_importance)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
    "unimportant_df = imp_df[imp_df['Importance'] <= imp_df['Importance'].quantile(0.5)]\n",
    "unimportant_cols = unimportant_df['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_df = train_final_df.drop(unimportant_cols, axis=1)\n",
    "print(train_selected_df.shape)\n",
    "train_selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_selected_df = test_final_df.drop(unimportant_cols, axis=1)\n",
    "print(test_selected_df.shape)\n",
    "test_selected_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_models_list2, cat_submission_df, cat_raw_train_preds, cat_raw_test_preds, cat_optuna_study, cat_shap_importance = catboost_param_tune(\n",
    "    train_selected_df, test_selected_df, 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in cat_optuna_study.best_params.keys():\n",
    "    fig = plot_slice(cat_optuna_study, params=[param])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(cat_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_optuna_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_selected_df.drop(['id', 'sii'], axis=1).columns\n",
    "imp_df = pd.DataFrame(sorted(zip(cols, cat_shap_importance)), columns=['Feature', 'Importance']).sort_values('Importance', ascending=False)\n",
    "unimportant_df = imp_df[imp_df['Importance'] <= imp_df['Importance'].quantile(0.5)]\n",
    "unimportant_cols = unimportant_df['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_df3 = train_selected_df.drop(unimportant_cols, axis=1)\n",
    "print(train_selected_df3.shape)\n",
    "train_selected_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_df3.to_csv(path + 'train_selectd_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_selected_df3.to_csv(path + 'test_selected_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_selected_df3 = test_selected_df.drop(unimportant_cols, axis=1)\n",
    "print(test_selected_df3.shape)\n",
    "test_selected_df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selected_df4 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_models_list3, cat_submission_df, cat_raw_train_preds, cat_raw_test_preds, cat_optuna_study, cat_shap_importance3 = catboost_param_tune(\n",
    "    train_selected_df3, test_selected_df3, 250\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "# Edited by Max (Jongyun Han)\n",
    "\n",
    "def lgb_feature_selection(train_data, test_data, optuna_n_trials):\n",
    "    X = train_data.drop(['id', 'sii'], axis=1)\n",
    "    y = train_data['sii']\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "\n",
    "    base_params = {\n",
    "        'verbosity': -1,\n",
    "        'seed': SEED,\n",
    "        'num_iterations': 10000,\n",
    "        'feature_fraction': 0.8,\n",
    "        'device': 'gpu'\n",
    "    }\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        params_to_tune = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'early_stopping_round': trial.suggest_int('early_stopping_round', 10, 70),\n",
    "            'max_depth': trial.suggest_int('max_depth', 6, 50),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 300),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 50, 10000),\n",
    "            'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0, 0.3),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 0, 10),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 50, 2000)\n",
    "        }\n",
    "\n",
    "        #for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            lgb_callbacks = [\n",
    "                lgb.log_evaluation(period=20)\n",
    "            ]\n",
    "    \n",
    "            model = LGBMRegressor(\n",
    "                **base_params,\n",
    "                **params_to_tune\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)])#, callbacks=lgb_callbacks)\n",
    "    \n",
    "            #plt.figure()\n",
    "            #lgb.plot_metric(model)\n",
    "            #plt.show()\n",
    "            \n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "    \n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "    \n",
    "        #print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "        #print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "        # https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use/discussion/551533\n",
    "        y_preds_df = pd.DataFrame({'y':y, 'preds':oof_non_rounded})\n",
    "        oof_initial_thresholds = y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=oof_initial_thresholds, args=(y, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "        del y_preds_df, oof_initial_thresholds\n",
    "        gc.collect()\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "    \n",
    "        #print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "        return tKappa\n",
    "\n",
    "    with tqdm(total=optuna_n_trials, desc=\"Optimizing\", unit=\"trial\") as pbar:\n",
    "        \n",
    "        # Define a callback function to update the progress bar\n",
    "        def progress_bar_callback(study, trial):\n",
    "            pbar.update(1)\n",
    "    \n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=optuna_n_trials, callbacks=[progress_bar_callback])\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        lgb_callbacks = [\n",
    "            lgb.log_evaluation(period=20)\n",
    "        ]\n",
    "\n",
    "        model = LGBMRegressor(\n",
    "            **base_params,\n",
    "            **best_params\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)])#, callbacks=lgb_callbacks)\n",
    "\n",
    "        plt.figure()\n",
    "        lgb.plot_metric(model)\n",
    "        plt.show()\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_y_preds_df = pd.DataFrame({'y':y_train, 'preds':y_train_pred})\n",
    "        train_oof_initial_thresholds = train_y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "\n",
    "        train_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=train_oof_initial_thresholds, args=(y_train, y_train_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert train_KappaOPtimizer.success, \"Train Optimization did not converge.\"\n",
    "\n",
    "        train_pred_tuned = threshold_Rounder(y_train_pred, train_KappaOPtimizer.x)\n",
    "        train_Kappa = quadratic_weighted_kappa(y_train, train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_y_preds_df = pd.DataFrame({'y':y_val, 'preds':y_val_pred})\n",
    "        val_oof_initial_thresholds = val_y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "\n",
    "        val_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=val_oof_initial_thresholds, args=(y_val, y_val_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert val_KappaOPtimizer.success, \"Val Optimization did not converge.\"\n",
    "\n",
    "        val_pred_tuned = threshold_Rounder(y_val_pred, val_KappaOPtimizer.x)\n",
    "        val_Kappa = quadratic_weighted_kappa(y_val, val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        train_optimized_val_pred_tuned = threshold_Rounder(y_val_pred, train_KappaOPtimizer.x)\n",
    "        train_optimized_val_Kappa = quadratic_weighted_kappa(y_val, train_optimized_val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized Val QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_optimized_val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_optimized_train_pred_tuned = threshold_Rounder(y_train_pred, val_KappaOPtimizer.x)\n",
    "        val_optimized_train_Kappa = quadratic_weighted_kappa(y_train, val_optimized_train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized Train QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_optimized_train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        rounded_train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        rounded_val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)        \n",
    "\n",
    "        train_S.append(train_Kappa)\n",
    "        test_S.append(val_Kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Rounded Train QWK: {rounded_train_kappa:.4f}, Rounded Validation QWK: {rounded_val_kappa:.4f}\")\n",
    "        #clear_output(wait=True)\n",
    "\n",
    "        models_list.append(model)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    y_preds_df = pd.DataFrame({'y':y, 'preds':oof_non_rounded})\n",
    "    oof_initial_thresholds = y_preds_df.groupby('y')['preds'].mean().iloc[1:].values.tolist()\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=oof_initial_thresholds, args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data_copy['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return models_list, submission, oof_non_rounded, tpm, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb_models_list, lgb_submission_df, lgb_raw_train_preds, lgb_raw_test_preds, lgb_optuna_study = lgb_feature_selection(train_final_df, test_final_df, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_dict = {\n",
    "    'verbosity': [-1],\n",
    "    'seed': [SEED],\n",
    "    'num_iterations': [10000],\n",
    "    'feature_fraction': [0.8],\n",
    "    'device': ['gpu'],\n",
    "    'learning_rate': [0.25260664212527056],\n",
    "    'early_stopping_round': [60],\n",
    "    'max_depth': [7],\n",
    "    'min_data_in_leaf': [183],\n",
    "    'num_leaves': [6388],\n",
    "    'min_gain_to_split': [0.05311446129934012],\n",
    "    'lambda_l1': [0.45136290968123227],\n",
    "    'lambda_l2': [804.4678501621871]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df = pd.DataFrame(lgb_params_dict)\n",
    "print(lgb_params_df.shape)\n",
    "lgb_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params_df.to_csv(path + 'lgb_params.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = test_selected_df8.columns.tolist()\n",
    "selected_cols_df = pd.DataFrame({'col':selected_cols})\n",
    "selected_cols_df.to_csv(path + 'selected_cols.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_models_list8, lgb_submission_df, lgb_raw_train_preds, lgb_raw_test_preds, lgb_optuna_study = lgb_feature_selection(\n",
    "    train_selected_df8, test_selected_df8, 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "# Edited by Max (Jongyun Han)\n",
    "def lgb_train(params, train_data, test_data):\n",
    "    X = train_data.drop(['id', 'sii'], axis=1)\n",
    "    y = train_data['sii']\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "\n",
    "    #for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "    for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        lgb_callbacks = [\n",
    "            lgb.log_evaluation(period=20)\n",
    "        ]\n",
    "\n",
    "        model = LGBMRegressor(\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)])#, callbacks=lgb_callbacks)\n",
    "\n",
    "        plt.figure()\n",
    "        lgb.plot_metric(model)\n",
    "        plt.show()\n",
    "        \n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(y_train, y_train_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert train_KappaOPtimizer.success, \"Train Optimization did not converge.\"\n",
    "\n",
    "        train_pred_tuned = threshold_Rounder(y_train_pred, train_KappaOPtimizer.x)\n",
    "        train_Kappa = quadratic_weighted_kappa(y_train, train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(y_val, y_val_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert val_KappaOPtimizer.success, \"Val Optimization did not converge.\"\n",
    "\n",
    "        val_pred_tuned = threshold_Rounder(y_val_pred, val_KappaOPtimizer.x)\n",
    "        val_Kappa = quadratic_weighted_kappa(y_val, val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        train_optimized_val_pred_tuned = threshold_Rounder(y_val_pred, train_KappaOPtimizer.x)\n",
    "        train_optimized_val_Kappa = quadratic_weighted_kappa(y_val, train_optimized_val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized Val QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_optimized_val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_optimized_train_pred_tuned = threshold_Rounder(y_train_pred, val_KappaOPtimizer.x)\n",
    "        val_optimized_train_Kappa = quadratic_weighted_kappa(y_train, val_optimized_train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized Train QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_optimized_train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        rounded_train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        rounded_val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)        \n",
    "\n",
    "        train_S.append(train_Kappa)\n",
    "        test_S.append(val_Kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Rounded Train QWK: {rounded_train_kappa:.4f}, Rounded Validation QWK: {rounded_val_kappa:.4f}\")\n",
    "        #clear_output(wait=True)\n",
    "\n",
    "        models_list.append(model)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data_copy['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return models_list, submission, oof_non_rounded, tpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "# Edited by Max (Jongyun Han)\n",
    "def catboost_feature_selection(train_data, test_data, optuna_n_trials=100):\n",
    "    X = train_data.drop(['id', 'sii'], axis=1)\n",
    "    y = train_data['sii']\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "\n",
    "    base_params = {\n",
    "        'iterations': 3000,\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'task_type': 'GPU',\n",
    "        'use_best_model': True,\n",
    "        'boosting_type': 'Plain'\n",
    "    }\n",
    "\n",
    "    def objective(trial):\n",
    "        \n",
    "        params_to_tune = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.03),\n",
    "            'depth': trial.suggest_int('depth', 6, 9),\n",
    "            'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 10, 70),\n",
    "            'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 20, 40)\n",
    "        }\n",
    "    \n",
    "        #for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "            model = CatBoostRegressor(\n",
    "                **base_params,\n",
    "                **params_to_tune\n",
    "            )\n",
    "            \n",
    "            model.fit(X_train, y_train, eval_set=(X_val, y_val))#, plot=True)\n",
    "    \n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "    \n",
    "            oof_non_rounded[test_idx] = y_val_pred\n",
    "            y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "            oof_rounded[test_idx] = y_val_pred_rounded\n",
    "    \n",
    "            train_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                      x0=[0.5, 1.5, 2.5], args=(y_train, y_train_pred), \n",
    "                                      method='Nelder-Mead')\n",
    "            assert train_KappaOPtimizer.success, \"Train Optimization did not converge.\"\n",
    "    \n",
    "            train_pred_tuned = threshold_Rounder(y_train_pred, train_KappaOPtimizer.x)\n",
    "            train_Kappa = quadratic_weighted_kappa(y_train, train_pred_tuned)\n",
    "    \n",
    "            #print(f\"----> || Train Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "            val_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                      x0=[0.5, 1.5, 2.5], args=(y_val, y_val_pred), \n",
    "                                      method='Nelder-Mead')\n",
    "            assert val_KappaOPtimizer.success, \"Val Optimization did not converge.\"\n",
    "    \n",
    "            val_pred_tuned = threshold_Rounder(y_val_pred, val_KappaOPtimizer.x)\n",
    "            val_Kappa = quadratic_weighted_kappa(y_val, val_pred_tuned)\n",
    "    \n",
    "            #print(f\"----> || Val Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "            train_optimized_val_pred_tuned = threshold_Rounder(y_val_pred, train_KappaOPtimizer.x)\n",
    "            train_optimized_val_Kappa = quadratic_weighted_kappa(y_val, train_optimized_val_pred_tuned)\n",
    "    \n",
    "            #print(f\"----> || Train Optimized Val QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_optimized_val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "            val_optimized_train_pred_tuned = threshold_Rounder(y_train_pred, val_KappaOPtimizer.x)\n",
    "            val_optimized_train_Kappa = quadratic_weighted_kappa(y_train, val_optimized_train_pred_tuned)\n",
    "    \n",
    "            #print(f\"----> || Val Optimized Train QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_optimized_train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "            rounded_train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "            rounded_val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)        \n",
    "    \n",
    "            train_S.append(train_Kappa)\n",
    "            test_S.append(val_Kappa)\n",
    "            \n",
    "            test_preds[:, fold] = model.predict(test_data)\n",
    "            \n",
    "            #print(f\"Fold {fold+1} - Rounded Train QWK: {rounded_train_kappa:.4f}, Rounded Validation QWK: {rounded_val_kappa:.4f}\")\n",
    "            #clear_output(wait=True)\n",
    "    \n",
    "            models_list.append(model)\n",
    "    \n",
    "        #print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "        #print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "    \n",
    "        KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "        \n",
    "        oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "        tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "    \n",
    "        print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "        tpm = test_preds.mean(axis=1)\n",
    "        tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "        \n",
    "        submission = pd.DataFrame({\n",
    "            'id': test_data_copy['id'],\n",
    "            'sii': tpTuned\n",
    "        })\n",
    "    \n",
    "        return tKappa\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=optuna_n_trials)  # You can increase n_trials for better results\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "catboost_optuna_study = catboost_feature_selection(train_final_df, test_final_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(catboost_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(catboost_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(catboost_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(catboost_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(catboost_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rank(catboost_optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ichigoe/lb0-494-with-tabnet\n",
    "# Edited by Max (Jongyun Han)\n",
    "def catboost_train(train_data, test_data):\n",
    "    X = train_data.drop(['id', 'sii'], axis=1)\n",
    "    y = train_data['sii']\n",
    "\n",
    "    test_data_copy = test_data.copy()\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    models_list = []\n",
    "\n",
    "    catboost_params = {\n",
    "        'learning_rate': 0.02,\n",
    "        'depth': 9,\n",
    "        'iterations': 2000,\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'l2_leaf_reg': 15,\n",
    "        'task_type': 'GPU',\n",
    "        'use_best_model': True,\n",
    "        'early_stopping_rounds': 30,\n",
    "        'boosting_type': 'Ordered'\n",
    "    }\n",
    "\n",
    "    #for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "    for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = CatBoostRegressor(\n",
    "            **catboost_params\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, eval_set=(X_val, y_val), plot=True)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(y_train, y_train_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert train_KappaOPtimizer.success, \"Train Optimization did not converge.\"\n",
    "\n",
    "        train_pred_tuned = threshold_Rounder(y_train_pred, train_KappaOPtimizer.x)\n",
    "        train_Kappa = quadratic_weighted_kappa(y_train, train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                                  x0=[0.5, 1.5, 2.5], args=(y_val, y_val_pred), \n",
    "                                  method='Nelder-Mead')\n",
    "        assert val_KappaOPtimizer.success, \"Val Optimization did not converge.\"\n",
    "\n",
    "        val_pred_tuned = threshold_Rounder(y_val_pred, val_KappaOPtimizer.x)\n",
    "        val_Kappa = quadratic_weighted_kappa(y_val, val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        train_optimized_val_pred_tuned = threshold_Rounder(y_val_pred, train_KappaOPtimizer.x)\n",
    "        train_optimized_val_Kappa = quadratic_weighted_kappa(y_val, train_optimized_val_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Train Optimized Val QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {train_optimized_val_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        val_optimized_train_pred_tuned = threshold_Rounder(y_train_pred, val_KappaOPtimizer.x)\n",
    "        val_optimized_train_Kappa = quadratic_weighted_kappa(y_train, val_optimized_train_pred_tuned)\n",
    "\n",
    "        print(f\"----> || Val Optimized Train QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {val_optimized_train_Kappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "        rounded_train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        rounded_val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)        \n",
    "\n",
    "        train_S.append(train_Kappa)\n",
    "        test_S.append(val_Kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Rounded Train QWK: {rounded_train_kappa:.4f}, Rounded Validation QWK: {rounded_val_kappa:.4f}\")\n",
    "        #clear_output(wait=True)\n",
    "\n",
    "        models_list.append(model)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_data_copy['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return models_list, submission, oof_non_rounded, tpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_models_list, cat_submission_df, cat_raw_train_preds, cat_raw_test_preds = catboost_train(train_final_df, test_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
